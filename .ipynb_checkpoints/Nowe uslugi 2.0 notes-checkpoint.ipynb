{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Nowe usługi 2.0. Przewodnik po analizie zbiorów danych\", Toby Segaran, 2014, Helion\n",
    "\n",
    "tagi: analiza danych, data analytics, uczenie maszynowe, machine learning\n",
    "-----\n",
    "\n",
    "inteligencja zbiorowa [s.22) - łączenie zachowań, preferencji lub pomysłów grupy osób w celu uzyskania nowatorskich spostrzeżeń\n",
    "\n",
    "uczenie maszynowe [s.23] - metoda wyciągania wcniosków z danych. Algorytm otrzymuje zbiór danych i określa wnioski dotyczące ich właściwości. Informacje te umożliwiają tworzenie przewidywań odnośnie do innych danych, które mogą pojawić się w przyszłości. Jest to możliwe, ponieważ niemal wszystkie nielosowe dane zawierają wzorce, które pozwalają maszynie dokonywać uogólnień. W tym celu trenowany jest model przy użyciu tego, co maszyna uzna za najważniejsze aspekty danych.\n",
    "\n",
    "drzewa decyzyjne a sieci neuronowe [s.23]\n",
    "\n",
    "rynki prognostyczne [s.25]\n",
    "\n",
    "# Tworzenie rekomendacji [s. 28]\n",
    "\n",
    "### Filtrowanie grupowe oparte na użytkownikach [s. 28] \n",
    "\n",
    "Filtrowanie grupowe - polega na przeszukiwaniu dużej grupy ludzi i znajdowaniu mniejszego zbioru osób o gustach podobnych do naszych. Algorytm sprawdza inne rzeczy lubiane przez te osoby i łączy je w celu utworzenia sklasyfikowanej listy sugestii.\n",
    "\n",
    "##### miary odległości [s. 29]\n",
    "\n",
    "miara odległości euklidesowej [s. 29] - określenie odległości między punktami w przestrzeni. Pierwiastek z sumy różnic podniesionych do kwadratu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![odleglosc euklidesowa](img/euclidean.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "współczynnik korelacji Pearsona [s. 31] - miara tego jak dobrze dwa zbiory danych dopasowane są do linii prostej. Zwykle daje lepsze wyniki niż miara długości euklidesowej w sytuacji gdy dane są dobrze znormalizowane.\n",
    "\n",
    "W jakich przypadkach warto użyć Pearsona? Tam gdzie istotna jest sama tendencja zmienności. [s. 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![korelacja Pearsona](img/kor_pearson.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykładowe miary odległości, które warto sprawdzić, to indeks/współczynnik Jaccarda, odległość Manhattan lub miara Tanimoto. [s. 33]\n",
    "Uwaga: w implementacji naszego kodu w Pythonie wyższa wartość powinna oznaczać wyższe podobieństwo\n",
    "\n",
    "\n",
    "##### znajdowanie pozycji podobnych [s. 33]\n",
    "\n",
    "średnia ważona (miernik syntetyczny) [s. 34] - polega na sumowaniu ocen innych osób przemnożonych przez miarę podobieństwa tych osób. Dzięki temu osoby o innym guście mają mniejszy wpływ na ocenę sumaryczną, np. filmu, który jest obiektem badania.\n",
    "\n",
    "##### dopasowywanie produktów [s. 36]\n",
    "Używając funkcji transformPrefs, można dokonać transformacji\n",
    "z postaci: {krytyk1: {film1: ocena1, film2: ocena2}, krytyk2: {film1: ocena1, film2: ocena2}}\n",
    "do postaci {film1: {krytyk1: ocena1, krytyk2: ocena2}, film2: {krytyk1: ocena1, krytyk2: ocena2}}\n",
    "Dzięki temu będzie można użyć tych samych funkcji do określenia podobieństwa filmów, a następnie sprawdzić kto polubił dany film i wyświetlić inne filmy, które przypadły tym osobom do gustu.\n",
    "\n",
    "### Filtrowanie grupowe oparte na pozycjach [s. 42]\n",
    "W przypadku bardzo dużych zbiorów danych ten typ filtrowania jest znacznie wydajniejszy i może dać lepsze wyniki.\n",
    "\n",
    "Jak to działa? Gdy pożądane jest utworzenie rekomendacji dla użytkownika, należy następnie sprawdzić jego najwyżej ocenione pozycje i wygenerować listę ważoną pozycji, które są najbardziej podobne do tych pozycji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykrywanie grup [s. 49]\n",
    "\n",
    "grupowanie danych [s.49] - metoda wykrywania i wizualizacji grup rzeczy, osób lub pomysłów, które są ze sobą blisko powiązane\n",
    "\n",
    "### Uczenie nadzorowane a nienadzorowane [s. 49]\n",
    "\n",
    "metody uczenia nadzorowanego - techniki wykorzystujące przykładowe wejścia (zbiór uczący: train set) i wyjścia (zbiór testujący: test set), np. sieci neuronowe, drzewa decyzyjne, maszyny wektorów nośnych, filtrowanie bayesowskie\n",
    "\n",
    "uczenie nienadzorowane [s. 50] - algorytmy uczenia nienadzorowanego nie są trenowane za pomocą przykładów z poprawnymi odpowiedziami. Ich celem jest znalezienie struktury w zbiorze danych, w którym żadna porcja danych nie jest odpowiedzią. Przykładem jest grupowanie. Jego celem jest pobranie danych i znalezienie różnych grup, które istnieją w ich obrębie. Inne przykłady nauczania nienadzorowanego to nieujemna faktoryzacja macierzy oraz samooorganizujące się mapy\n",
    "\n",
    "#### Liczenie wyrazów w kanale informacyjnym [s. 51]\n",
    "Dane zostały przetworzone do postaci pliku txt rozdzielanego tabem, zawierającego listę blogów (adresów URL) i liczebności znajdujących się w nich słów.\n",
    "![blog data for clustering](img/blogdata.jpg)\n",
    "\n",
    "## Grupowanie hierarchiczne [s. 53]\n",
    "hierarchical clustering\n",
    "\n",
    "Grupowanie hierarchiczne - tworzy hierarchię grup poprzez ciągłe scalanie dwóch najbardziej podobnych. Na początku każda z tych grup jest pojedynczą pozycją. W każdej iteracji metoda oblicza odległości między każdą parą grup. Najbliższe pary są ze sobą scalane w celu utworzenia nowej grupy. Jest to powtarzane do momentu pozostania tylko jednej grupy.\n",
    "![hierarchical clustering](img/hierarchical_clustering.jpg)\n",
    "\n",
    "Po zakończeniu grupowania hierarchicznego wyniki są zwykle prezentowane w formie diagramu nazywanego dendrogramem.\n",
    "\n",
    "#### Algorytm grupowania hierarchicznego\n",
    "1. Załadowanie pliku z danymi i ekstrakcja ich do trzech list.\n",
    "2. Zdefiniowanie bliskości (np. korelacja Pearsona) w formie funkcji obliczającej odległość między obiektami.\n",
    "3. Utworzenie klasy przechowującej informacje o klastrach (połączeniach) - bicluster.\n",
    "4. Wyszukiwanie najbliższych obiektów (funkcja hcluster) i krokowe łączenie ich w grupy aż do pozostania jednej grupy. W wyniku tej funkcji w pamięci operacyjnej przechowywyane są wszystkie oryginalne obiekty i tworzone po kolei klastry wraz z atrybutami (lewa gałąź, prawa gałąź, wartości cech, id, odległość między obiema gałęziami).\n",
    "\n",
    "#### Funkcja grupowania - hcluster\n",
    "1. Utworzenie z każdego wiersza instancji klasy bicluster (na początku każdy obiekt (blog) stanowi osobną grupę/klaster).\n",
    "2. Iterowanie po wszystkich możliwych parach. Jeśli dystans między klastrami nie został jeszcze obliczony (sprawdzane w słowniku). obliczamy go i dodajemy do słownika.\n",
    "3. Porównanie długości i wyłonienie najmniejszej.\n",
    "4. Obliczenie danych nowego klastra (średnia dwóch poprzednich klastrów) i utworzenie nowego klastra (obiektu klasy bicluster).\n",
    "5. Nowy klaster dołączany jest do listy klastrów by podlegać sprawdzaniu w kolejnych iteracjach.\n",
    "6. Jeśli został tylko jeden klaster, staje się wynikiem funkcji. W innym razie pętla wraca do punktu 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bicluster:\n",
    "    def __init__(self, vec, left=None, right=None, distance=0.0, id=None):\n",
    "        self.left = left # left branch\n",
    "        self.right = right # right branch\n",
    "        self.vec = vec # wektor danych - wiersz z danymi\n",
    "        self.id = id\n",
    "        self.distance = distance # distance from branches (left & right)\n",
    "\n",
    "    def __call__(self):\n",
    "        print(self.left, self.right, self.vec, self.id, self.distance)\n",
    "\n",
    "def hcluster(rows, distance=pearson):\n",
    "    distances = {}\n",
    "    currentclustid = -1\n",
    "\n",
    "    # Clusters are initially just the rows\n",
    "    clust = [bicluster(vec=rows[i], id=i) for i in range(len(rows))]\n",
    "\n",
    "    while len(clust) > 1: # pętla wykonywana jest tak długo aż pozostanie tylko jeden klaster (czyli jeden obiekt w liście clust)\n",
    "        lowestpair = (0, 1) # tworzymy sztuczną krotkę, która zostania nadpisana po obliczeniu pary klastrów o najmniejszej odległości\n",
    "        closest = distance(clust[0].vec, clust[1].vec) # tworzymy sztuczną zmienną, która jest potrzebna do pierwszego\n",
    "            # porównania odległości. Później zostanie nadpisana, jeśli pojawi się mniejsza odległość między obiektami\n",
    "\n",
    "    # loop through every pair looking for the smallest distance\n",
    "        for i in range(len(clust)):\n",
    "            for j in range(i + 1, len(clust)):\n",
    "                # distances is the cache of distance calculations\n",
    "                if (clust[i].id, clust[j].id) not in distances:\n",
    "                    distances[(clust[i].id, clust[j].id)] = \\\n",
    "                        distance(clust[i].vec, clust[j].vec)\n",
    "\n",
    "                d = distances[(clust[i].id, clust[j].id)]\n",
    "\n",
    "                if d < closest:\n",
    "                    closest = d\n",
    "                    lowestpair = (i, j)\n",
    "\n",
    "        # calculate the average of the two clusters\n",
    "        # this will be the data for new cluster based on these two branches\n",
    "        mergevec = [(clust[lowestpair[0]].vec[i] + clust[lowestpair[1]].vec[i])\n",
    "                    / 2.0 for i in range(len(clust[0].vec))]\n",
    "\n",
    "        # create the new cluster\n",
    "        newcluster = bicluster(mergevec, left=clust[lowestpair[0]],\n",
    "                               right=clust[lowestpair[1]], distance=closest,\n",
    "                               id=currentclustid)\n",
    "\n",
    "        # cluster ids that weren't in the original set are negative\n",
    "        currentclustid -= 1\n",
    "        del clust[lowestpair[1]]\n",
    "        del clust[lowestpair[0]]\n",
    "        clust.append(newcluster)\n",
    "\n",
    "    return clust[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uwaga:\n",
    "Jeśli masz więcej obiektów niż zmiennych, sensowność utworzonych grup może być wątpliwa. [s. 61]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grupowanie metodą k-średnich [s. 61]\n",
    "Grupowanie metodą k-średnich to grupowanie niehierarchiczne, w którym ilość grup jest z góry założona. Algorytm ten jest znacznie wydajniejszy niż algorytm grupowania hierarchicznego. \n",
    "\n",
    "#### Algorytm grupowania metodą k-średnich\n",
    "1. Grupowanie metodą k-średnich rozpoczyna się od kilku losowo umieszczonych centroidów.\n",
    "2. Każdy element przypisywany jest do najbliższego centroidu.\n",
    "3. Centroid przesuwany jest do średniej pozycji wszystkich przypisanych do tegoż centroidu obiektów.\n",
    "4. Punkty 2 i 3 są powtarzane tak długo aż nowe przypisania przestaną powodować jakiekolwiek zmiany.\n",
    "\n",
    "![k-means clustering](img/k-means_clustering.jpg)\n",
    "\n",
    "#### Współczynnik Tanimoto\n",
    "odległość Tanimoto - miara zbieżności, która nadaje się do opisu obiektów o cesze 0/1-owej. Jest to stosunek przecięcia zbiorów (zawiera wyłącznie pozycje obecne w obu zbiorach) i sumy zbiorów (wszystkie pozycje dowolnego ze zbiorów)\n",
    "\n",
    "\n",
    "## Skalowanie wielowymiarowe - wyświetlanie danych w dwóch wymiarach (p. 49 / s. 68)\n",
    "multidimensional scaling\n",
    "W rzeczywistych przykładach pozycji, które miałyby zostać pogrupowane, występują zwykle więcej niż dwie liczby. Nie można zatem użyć danych w niezmienionej postaci i przedstawić ich na wykresie dwuwymiarowym.\n",
    "\n",
    "Jako że dwuwymiarowy wykres ułatwia zrozumienie relacji między różnymi pozycjami, można posłużyć się skalowaniem wielowymiarowym do prezentacji relacji między obiektami poprzez odległość między nimi na wykresie dwuwymiarowym."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
